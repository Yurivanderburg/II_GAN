\section{Generative Adversarial Networks}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig/analysis/Discrep_Runs_Losses_disc_loss.png}
	\includegraphics[width=\linewidth]{fig/analysis/Discrep_Runs_Losses_gen_total_loss.png}
	\caption{\textcolor{blue}{These figures show the effect of the ratio of episodes of Discriminator training per every episode of Generator training. This hyperparameter is termed Discriminator repetition or (discrep) in the figures. The square root of the cumulative mean of the losses are plotted against the training steps for 3 values of this ratio. Understandably, this ratio has a higher impact on the Discriminator loss than the Generator loss. Equal number of episodes of training produces minimum cumulative loss of the Discriminator. The dip in the Discriminator loss during the initial phases of its training can be interpreted as its early success in detecting ``fake" (or generated) images because of a poorly trained Generator. With gradual training of the Generator, the success rate of the Discriminator decreases and eventually approaches saturation with equal probability of being successful in telling ``fake" from real. The continual decrease and eventual saturation of the Generator loss is a result of its training to generate better images with increasing number of steps.}}
	\label{fig:Plot_discrep_loss}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig/analysis/Telescope_Runs_Losses_disc_loss.png}
	\includegraphics[width=\linewidth]{fig/analysis/Telescope_Runs_Losses_gen_total_loss.png}
	\caption{The square root of cumulative mean of Discriminator and Generator loss for different numbers of telescopes. The number of telescopes is also another hyperparameter that has significant impact on the model performance. If there are only two telescopes, both Discriminator and Generator are not trained smoothly. The result of four telescopes is a lot better because the cumulative mean of loss functions is smaller compare to other parameters. For the same reasons as explained in Fig.(\ref{fig:Plot_discrep_loss}), we observe initial dip and eventual saturation of the Discriminator loss and continual decrease and eventual saturation of the Generator loss with training steps. }
	\label{fig:Plot_telescopes_loss}
\end{figure}
Generative Adversarial Networks (GANs) were introduced by \cite{goodfellow2014generative}. A GAN model involves two competing deep neural network models, referred to as the Generator and the Discriminator. These two networks engage in a zero-sum “Minimax” game, as in the Game Theory. Given a real data set $\{x_i\}$ (for example, a set of real images) drawn from some unknown distribution $P_{\mathrm{data}}(x)$ generated by some unknown or ill-understood process, the objective here is to generate a new set (of images) whose probability distribution should match $P_{\mathrm{data}}(x)$ as closely as possible. During the training of the two models, the Generator samples a latent variable \(z\) from a known prior distribution \(P_z(z)\) ({\it e.g.}, the Normal Distribution) and produces a synthetic sample \(G(z)\) to start with and, subsequently, based on updates received from the Discriminator as its training progresses. The Discriminator, being a probabilistic binary classifier, receives either a real data sample \(x\) or a generated sample \(G(z)\), and outputs a probability that the input is real. The Discriminator aims to maximise its classification accuracy, while the Generator aims to fool it by trying to minimize it. The training of these two networks proceeds alternately leading to the optimization of the adversarial loss function $L(D,G)$ given by 
\begin{equation}
	\centering
	\begin{aligned}
		L(D,G) & = \min_{G} \max_{D} V(D, G)& \\
		&= \mathbb{E}_{x \sim p_{\rm data}(x)} \left[ \log D(x) \right] \\
		&+ \mathbb{E}_{z \sim p_{z}(z)} \left[ \log \left( 1-D(G(z)) \right) \right].
	\end{aligned}
	\label{eq:Basic_GAN}
\end{equation}
The two neural networks \( G(z) \equiv G(z; \theta_G)\) and \(D(x) \equiv D(x; \theta_D)\) are parameterized by \(\theta_i\) with \(i = G \ \mathrm{or}\ D\) respectively. During its training, the Generator generates the differentiable function $x_{\mathrm{gen}} \equiv G(z)$ that maps \(z\) to the data space \(x\). Through such maps the Generator generates its own distribution $p_{\mathrm G}(x_{\mathrm gen})$ and through the training episodes, specifically by iteratively updating its parameters $\theta_{\mathrm G}$, aligns this distribution with the distribution of real data $p_{\mathrm data}(x)$. The training data set provided to the Discriminator is constructed by mixing real data points $x$ and generated data points $x_{\mathrm gen}$ in equal ratio. The Discriminator generates the function \(D(x)\) that represents the probability that \(x\) originates from real data. Eq.(\ref{eq:Basic_GAN}) implies that training of the GAN model moves towards maximization of the expectation of \(D(x)\). Through this process, both the parameters $\theta_{\mathrm G}$ and $\theta_{\mathrm D}$ are optimized such that $p_{\mathrm G}(x_{\mathrm gen})$ gets maximally aligned with $p_{\mathrm data}(x)$.  

For a given fixed Generator $G(z)$, the problem can be reformulated as:
\begin{equation}
	\centering
	\begin{aligned}
		\max_{D} V(D,G) &= \mathbb{E}_{x \sim p_{\rm data}} \left[ \log D^{*}_{G}(x) \right] \\ 
		&+ \mathbb{E}_{x \sim p_{G}} \left[ \log \left( 1 - D^{*}_{G}(x) \right) \right]
	\end{aligned}
	\label{eq:GAN_reformulated}
\end{equation}
where \(D^{*}_{G}\) denotes the optimum of the discriminator. As seen in equation (\ref{eq:Disc_optimum}), the global optimum of equation (\ref{eq:GAN_reformulated}) is achieved if and only if \(p_G = p_{\rm data}\). Furthermore, if both \(G\) and \(D\) are allowed to reach their respective optima, -- the so called Nash point of the Minimax game -- \(p_{\mathrm G}\) converges to \(p_{\rm data}\). 
\begin{equation}
	\centering
	D^*_G(x) = \frac{p_{\rm data}(x)}{p_{\rm data}(x) + p_G(x_{\mathrm gen})}
	\label{eq:Disc_optimum}
\end{equation}
At this point, the Discriminator finds its job no better than random guessing. A more comprehensive discussion of the problem, including proofs, is provided in \cite{goodfellow2014generative}.

Subsequently, the GAN framework was extended to a conditional model \citep{mirza2014conditional}. This new model, known as ``conditional GAN" or cGAN injects a conditioning variable \(y\) into both networks: the Generator now generates \(G(z \mid y)\), and the Discriminator evaluates \(D(x \mid y)\). The adversarial objective becomes  
\begin{equation}
	\centering
	\begin{aligned}
		V(D, G) &= \mathbb{E}_{x,y \sim p_{\rm data}(x)} \left[ \log D(x|y) \right] \\
		&+ \mathbb{E}_{z,y\sim p_{z}(z)} \left[ \log \left( 1-D(G(z|y) \right) \right]
	\end{aligned}
	\label{eq:conditional_GAN}
\end{equation}
In our work, we specifically use this conditioning mechanism by choosing \(y\) to represent the ground-based intensity-interferometry (II) observation patterns: the Generator is trained to produce stellar surface images that not only look realistic but also conform to the measured II correlations, while the Discriminator judges realism \emph{and} consistency with the II data. 

\iffalse 
During each training session of the Discriminator, it is presented with a series of real images (as seen in Fig. (\ref{fig:GANinput}) and generated images taken in equal proportion. During the initial phases of its training, as the generated images are sampled from random distribution, the Discriminator finds it easy to distinguish between the real and the generated images. This produces a sharp initial dip in the Discriminator loss function. On the other hand, the Generator loss function, during the initial phases of its training is high due to the poor quality of its generated images. Through the process of alternating training sessions, the Generator gradually learns to do better: generate images which resemble the real images ever more closely. As seen in fig. (\ref{fig:Plot_discrep_loss}), with further training of both the networks, the Generator loss is gradually minimized and the generated images become gradually indistinguishable from the real images. The Discriminator loss, on the other hand, increases from the minimum of the initial phases of training and gradually  saturates to a value halfway between its maximum and minimum values, as expected in the case of a binary classifier.}
\fi

\cite{isola2017image} further observed that combining the cGAN from Eq.~\eqref{eq:conditional_GAN} with the traditional $L_1(G)$ loss \textcolor{blue}{(also known  as the Mean Absolute Error) improves the results, as the Generator is encouraged to produce outputs closer to the target image in a pixel-wise sense. We adopt this approach in the training of our cGAN model by including $L_1(G)$ along with the hyperparameter $\lambda =100$. The total loss function that is minimized is}:
\begin{equation}
	\centering
	L_{tot} = \arg \min_{G} \max_{D} V(D, G) + \lambda \cdot L_1(G)
	\label{eq:total_gen_loss}
\end{equation}
with the choice $\lambda = 100$ and
\begin{equation}
	\centering
	L_1(G) = \mathbb{E}_{x,y,z} \left[ ||{y - G(x,z)}||_1 \right]
	\label{eq:l1_loss}
\end{equation}
This type of network has demonstrated remarkable robustness across a variety of applications. For example, it can generate colored images from grayscale inputs based on architectural labels, transform images from day to night, and even predict maps from satellite data. A more extensive list of applications is provided in \cite{isola2017image}.

\subsection{Generator}
As discussed above, in a GAN the Generator, a deep neural network in iteself, is responsible for producing synthetic data — in this case, images that resemble those of a fast-rotating star. In this work, the Generator is implemented as a U-Net convolutional network \citep{ronneberger2015u}. \textcolor{blue}{The U-Net consists of a symmetric encoder--decoder structure forming a characteristic ``U'' shape: the left (contracting or down-sampling) path, the right (expanding or up-sampling path) and the connecting (bottle-neck) path. The left down-sampling path repeatedly applies 3$\times$3 convolutions (padded to preserve spatial dimensions) followed by LeakyReLU activations and strided convolution (with stride of 2), by progressively halving the spatial resolution while doubling the channel depth (typically 64 $\to$ 128 $\to$ 256 $\to$ 512 $\to$ 1024). At the bottleneck, high-level features are processed without further spatial reduction. The right up-sampling path mirrors this process using 2$\times$2 transposed convolutions (stride 2) for up-sampling, halving the channel count at each level and using ReLU as the activation function for all its layers except the output layer. Additionally, a dropout layer is introduced at the beginning of the upsampling phase to mitigate overfitting of the Generator model \citep{isola2017image}. Critically, long skip connections concatenate feature maps from each encoder (down-sampling) level to the corresponding decoder (up-sampling level), directly injecting high-resolution details into the reconstruction process. This enables the network to ``remember where everything is'' while the deep bottleneck still provides the large receptive field needed to think globally about image structure and semantics. Besides the strided convolutions, modern variants of U-Nets used in state-of-the-art GANs incorporate residual blocks within resolution levels, and frequently add spectral normalization and self-attention at the bottleneck for improved training stability and long-range dependency modelling. These architectural choices allow our U-Net generator to simultaneously recover sharp, high-frequency details (stellar limb edges, limb-darkening profiles, gravity darkening, and rapid-rotation-induced oblateness) and enforce global coherence (overall disk morphology and physical consistency with the observed interferometric visibilities)---making it ideally suited for high-fidelity sky-image reconstruction of fast-rotating stars from sparse ground-based intensity interferometry observations.}

After generating images, the Generator aims to deceive the Discriminator into classifying the generated images as real. The extent to which the Generator succeeds in this deception is quantified by the GAN loss. When the Discriminator is unable to distinguish between the generated and real images (i.e., when the GAN loss is minimized), the Generator is considered to have reached an optimal state. Conversely, if the generated image fails to fool the Discriminator, the Generator produces a new image for further comparison with the real image. Additionally, the Generator's performance is evaluated using another metric known as the L1 loss, which is defined as the mean absolute error between the pixels of the real image and those of the generated image. Balancing the minimization of both the GAN loss and the L1 loss enables the Generator to produce images that are not only realistic but also faithful to the input data. 

\subsection{Discriminator}
The Discriminator is tasked with classifying the images produced by the Generator as either real or fake. It takes a real image from the dataset (often referred to as the target image for the Generator) and provides feedback to guide the Generator toward producing more accurate images. In this work, the PatchGAN model \citep{isola2017image} is employed as the Discriminator. Unlike a traditional global classifier, PatchGAN evaluates individual patches of the image, outputting a grid of predictions rather than a single scalar value. \textcolor{blue}{Each element in the grid corresponds to the ``realness'' of one patch of the image under examination of the Discriminator at a time. The final loss is of the Discriminator is the average over all the patch responses. Evaluating the ``realness'' of the input image in terms of its constituent patches facilitates capture of texture/ style and other high frequency components in the image. As compared to a global discriminator, it also reduces the number of parameters in the network thereby helping reduce computation cost. It also works on images with arbitrary sizes.}

The Discriminator’s architecture begins with an initializer that accepts both the input (generated) images and the corresponding real images. Initially, Salt-and-Pepper noise is added to the input images. PatchGAN then reduces the spatial dimensions of the images to extract localized features, ensuring the model focuses on smaller regions. In this downsampling stage, a leaky version of the Rectified Linear Unit (LeakyReLU) is applied in the convolutional layers, similar to the approach used in the Generator.

Subsequently, zero padding is applied, adding rows and columns of zeros around the images to prevent the loss of spatial information during convolution and to facilitate the extraction of deeper features from the downsampled output. Following this, batch normalization is employed to stabilize learning by normalizing activations, and the Discriminator begins classifying each patch as real or fake. This is followed by additional layers involving LeakyReLU activation, zero padding, and convolution, culminating in a final prediction that is fed to the Generator as feedback.

The effectiveness of the Discriminator is measured by its ability to distinguish between real and fake images, quantified through the Discriminator loss. This loss is composed of two parts: one that measures how accurately the Discriminator identifies real images (by comparing predictions to a target value of 1) and another that assesses how accurately it identifies fake images (by comparing predictions to a target value of 0). Together, these loss components ensure that the Discriminator improves its classification performance, which in turn, challenges the Generator to produce increasingly realistic images.
