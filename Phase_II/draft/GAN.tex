\section{Generative Adversarial Networks}
\begin{figure*}
\centering
\includegraphics[width=0.75\linewidth]{fig/FlowDiagram_TikZ.png}
\caption{A schematic representation of the features of a cGAN model used in this work and the iterative process of its training, validation and testing. The process constitutes four broad stages: (1) choice of an appropriate GAN architecture including both the Discriminator and the Generator (not shown in this figure) (2) preparation of the Training, Validation and Testing datasets and (3) Training and Validation of the Model (4) Testing and Evaluation of the Model. The stages (2), (3) and (4) are depicted in this figure. The datasets are prepared in three broad steps: (i) generating the ``ground truth'' images of fictitious fast-rotators $x$, the sparse II images $y$ used as the ``condition'' images in the Model and the generated images $z$ sampled from a Normal Distribution (ii)merging these images into individual files with $(x|y)$ and/ or $(z|y)$ (as seen in an illustrated sample in Fig.(\ref{fig:GANinput})) and generating the full dataset in this process, and finally (iii) partitioning the full dataset into Training Set, Validation Set and the Testing Set. After the iterative training of the Model and its validation process is complete (``Nash point'' of the Minimax Game is reached), the Model is tested using the Testing Set and evaluated.}
\label{fig:FlowDiagram}
\end{figure*}
 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig/analysis/Discrep_Runs_Losses_disc_loss.png}
	\includegraphics[width=\linewidth]{fig/analysis/Discrep_Runs_Losses_gen_total_loss.png}
	\caption{These figures show the effect of the ratio of episodes of Discriminator training per every episode of Generator training. This hyperparameter is termed Discriminator repetition or (discrep) in the figures. The square root of the cumulative mean of the losses are plotted against the training steps for 3 values of this ratio. Understandably, this ratio has a higher impact on the Discriminator loss than the Generator loss. Equal number of episodes of training produces minimum cumulative loss of the Discriminator. The dip in the Discriminator loss during the initial phases of its training can be interpreted as its early success in detecting ``fake" (or generated) images because of a poorly trained Generator. With gradual training of the Generator, the success rate of the Discriminator decreases and eventually approaches saturation with equal probability of being successful in telling ``fake" from real. The continual decrease and eventual saturation of the Generator loss is a result of its training to generate better images with increasing number of steps.}
	\label{fig:Plot_discrep_loss}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig/analysis/Telescope_Runs_Losses_disc_loss.png}
	\includegraphics[width=\linewidth]{fig/analysis/Telescope_Runs_Losses_gen_total_loss.png}
	\caption{The square root of cumulative mean of Discriminator and Generator loss for different numbers of telescopes. The number of telescopes is also another hyperparameter that has significant impact on the model performance. If there are only two telescopes, both Discriminator and Generator are not trained smoothly. The result of four telescopes is a lot better because the cumulative mean of loss functions is smaller compare to other parameters. For the same reasons as explained in Fig.(\ref{fig:Plot_discrep_loss}), we observe initial dip and eventual saturation of the Discriminator loss and continual decrease and eventual saturation of the Generator loss with training steps. }
	\label{fig:Plot_telescopes_loss}
\end{figure}
Generative Adversarial Networks (GANs) were introduced by \cite{goodfellow2014generative}. A GAN model involves two competing deep neural network models, referred to as the Generator and the Discriminator. These two networks engage in a zero-sum “Minimax” game, as in the Game Theory. Given a real data set $\{x_i\}$ (for example, a set of real images) drawn from some unknown distribution $P_{\mathrm{data}}(x)$ generated by some unknown or ill-understood process, the objective here is to generate a new set (of images) whose probability distribution should match $P_{\mathrm{data}}(x)$ as closely as possible. During the training of the two models, the Generator samples a latent variable \(z\) from a known prior distribution \(P_z(z)\) ({\it e.g.}, the Normal Distribution) and produces a synthetic sample \(G(z)\) to start with and, subsequently, based on updates received from the Discriminator as its training progresses. The Discriminator, being a probabilistic binary classifier, receives either a real data sample \(x\) or a generated sample \(G(z)\), and outputs a probability that the input is real. The Discriminator aims to maximise its classification accuracy, while the Generator aims to fool it by trying to minimize it. The training of these two networks proceeds alternately leading to the optimization of the adversarial loss function $L(D,G)$ given by 
\begin{equation}
	\centering
	\begin{aligned}
		L(D,G) & = \min_{G} \max_{D} V(D, G)& \\
		&= \mathbb{E}_{x \sim p_{\rm data}(x)} \left[ \log D(x) \right] \\
		&+ \mathbb{E}_{z \sim p_{z}(z)} \left[ \log \left( 1-D(G(z)) \right) \right].
	\end{aligned}
	\label{eq:Basic_GAN}
\end{equation}
The two neural networks \( G(z) \equiv G(z; \theta_G)\) and \(D(x) \equiv D(x; \theta_D)\) are parameterized by \(\theta_i\) with \(i = G \ \mathrm{or}\ D\) respectively. During its training, the Generator generates the differentiable function $x_{\mathrm{gen}} \equiv G(z)$ that maps \(z\) to the data space \(x\). Through such maps the Generator generates its own distribution $p_{\mathrm G}(x_{\mathrm gen})$ and through the training episodes, specifically by iteratively updating its parameters $\theta_{\mathrm G}$, aligns this distribution with the distribution of real data $p_{\mathrm data}(x)$. The training data set provided to the Discriminator is constructed by mixing real data points $x$ and generated data points $x_{\mathrm gen}$ in equal ratio. The Discriminator generates the function \(D(x)\) that represents the probability that \(x\) originates from real data. Eq.(\ref{eq:Basic_GAN}) implies that training of the GAN model moves towards maximization of the expectation of \(D(x)\). Through this process, both the parameters $\theta_{\mathrm G}$ and $\theta_{\mathrm D}$ are optimized such that $p_{\mathrm G}(x_{\mathrm gen})$ gets maximally aligned with $p_{\mathrm data}(x)$.  

For a given fixed Generator $G(z)$, the problem can be reformulated as:
\begin{equation}
	\centering
	\begin{aligned}
		\max_{D} V(D,G) &= \mathbb{E}_{x \sim p_{\rm data}} \left[ \log D^{*}_{G}(x) \right] \\ 
		&+ \mathbb{E}_{x \sim p_{G}} \left[ \log \left( 1 - D^{*}_{G}(x) \right) \right]
	\end{aligned}
	\label{eq:GAN_reformulated}
\end{equation}
where \(D^{*}_{G}\) denotes the optimum of the discriminator. As seen in equation (\ref{eq:Disc_optimum}), the global optimum of equation (\ref{eq:GAN_reformulated}) is achieved if and only if \(p_G = p_{\rm data}\). Furthermore, if both \(G\) and \(D\) are allowed to reach their respective optima, -- the so called Nash point of the Minimax game -- \(p_{\mathrm G}\) converges to \(p_{\rm data}\). 
\begin{equation}
	\centering
	D^*_G(x) = \frac{p_{\rm data}(x)}{p_{\rm data}(x) + p_G(x_{\mathrm gen})}
	\label{eq:Disc_optimum}
\end{equation}
At this point, the Discriminator finds its job no better than random guessing. A more comprehensive discussion of the problem, including proofs, is provided in \cite{goodfellow2014generative}.

Subsequently, the GAN framework was extended to a conditional model \citep{mirza2014conditional}. This new model, known as ``conditional GAN" or cGAN injects a conditioning variable \(y\) into both networks: the Generator now generates \(G(z \mid y)\), and the Discriminator evaluates \(D(x \mid y)\). The adversarial objective becomes  
\begin{equation}
	\centering
	\begin{aligned}
		V(D, G) &= \mathbb{E}_{x,y \sim p_{\rm data}(x)} \left[ \log D(x|y) \right] \\
		&+ \mathbb{E}_{z,y\sim p_{z}(z)} \left[ \log \left( 1-D(G(z|y)|y )\right) \right]
	\end{aligned}
	\label{eq:conditional_GAN}
\end{equation}
The conditional variable $y$ in a cGAN can be various additional information including images, labels or text contextual to ``ground truth'' real data $x$. Among various types of cGANs, Pix2Pix GAN with its image-to-image translation design is suitable for the task at hand.
In our work, we specifically use this conditional variable by choosing \(y\) to represent the ground-based intensity-interferometry (II) observation patterns: the Generator is trained to produce stellar surface images that not only look realistic but also conform to the measured II correlations, while the Discriminator judges realism \emph{and} consistency with the II data.

\cite{isola2017image} further observed that combining the cGAN from Eq.~\eqref{eq:conditional_GAN} with the traditional $L_1(G)$ loss (also known  as the Mean Absolute Error) improves the results, as the Generator is encouraged to produce outputs closer to the target image in a pixel-wise sense. We adopt this approach in the training of our cGAN model by including $L_1(G)$ defined in eq.(\ref{eq:l1_loss})
\begin{equation}
	\centering
	L_1(G) = \mathbb{E}_{x \sim p_{\mathrm{data}, z \sim p_z(z)}} \bigl[ \lVert x - G(z \mid y) \rVert_{1} \bigr].
	\label{eq:l1_loss}
\end{equation}
The total adversarial loss function, along with the $L_1$ loss modulated by a hyperparameter $\lambda$ then becomes
\begin{equation}
	\centering
	L_{tot} = \arg \min_{G} \max_{D} V(D, G) + \lambda \cdot L_1(G).
	\label{eq:total_gen_loss}
\end{equation}

This type of network has demonstrated remarkable robustness across a variety of applications. For example, it can generate colored images from grayscale inputs based on architectural labels, transform images from day to night, and even predict maps from satellite data. A more extensive list of applications is provided in \cite{isola2017image}.

\subsection{Generator}
As discussed above, in a GAN the Generator, a deep neural network in iteself, is responsible for producing synthetic data, in this case, images that resemble those of a fast-rotating star. In this work, the Generator is implemented as a U-Net convolutional network \citep{ronneberger2015u}. The U-Net consists of a symmetric encoder--decoder structure forming a characteristic ``U'' shape along with skip connections: the left (contracting or down-sampling) path, the right (expanding or up-sampling path) and the connecting (bottle-neck) path. The left down-sampling path repeatedly applies 3$\times$3 convolutions (padded to preserve spatial dimensions) followed by LeakyReLU activations and strided convolution (with stride of 2), by progressively halving the spatial resolution while doubling the channel depth (typically 64 $\to$ 128 $\to$ 256 $\to$ 512 $\to$ 1024). At the bottleneck, high-level features are processed without further spatial reduction. The right up-sampling path mirrors this process using 2$\times$2 transposed convolutions (stride 2) for up-sampling, halving the channel count at each level and using ReLU as the activation function for all its layers except the output layer. Additionally, a dropout layer is introduced at the beginning of the upsampling phase to mitigate overfitting of the Generator model \citep{isola2017image}. Critically, long skip connections concatenate feature maps from each encoder (down-sampling) level to the corresponding decoder (up-sampling level), directly injecting high-resolution details into the reconstruction process. This enables the network to ``remember where everything is'' while the deep bottleneck still provides the large receptive field needed to think globally about image structure and semantics. Besides the strided convolutions, modern variants of U-Nets used in state-of-the-art GANs incorporate residual blocks within resolution levels, and frequently add spectral normalization and self-attention at the bottleneck for improved training stability and long-range dependency modelling. These architectural choices allow our U-Net generator to simultaneously recover sharp, high-frequency details (stellar limb edges, limb-darkening profiles, gravity darkening, and rapid-rotation-induced oblateness) and enforce global coherence (overall disk morphology and physical consistency with the observed interferometric visibilities)---making it ideally suited for high-fidelity sky-image reconstruction of fast-rotating stars from sparse ground-based intensity interferometry observations.

During the training, the total Generator Loss function $L_G$ including the $L_1$ loss defined in eq.(\ref{eq:l1_loss}) that is minimized is given by
\begin{equation}
\centering
\begin{aligned}
L_{G} \; &= \; - \, \mathbb{E}_{z \sim p_{z}(z)} \bigl[ \log D(G(z \mid y)\mid y) \bigr] \\
& +\; \lambda \; \mathbb{E}_{x \sim p_{\mathrm{data}, z \sim p_z(z)}} \bigl[ \lVert x - G(z \mid y) \rVert_{1} \bigr].  
\end{aligned}
\label{eq:total_gen_loss}
\end{equation}
Here, 
\begin{itemize}
  \item \(x\) denotes a real data sample (e.g.\ the ``ground-truth'' image; here, the synthetically generated fast rotator image) corresponding to condition \(y\), the simulated II observation data.  
  \item \(z\) is a random latent vector drawn from the prior \(p_z(z)\).  
  \item \(G(z \mid y)\) is the generator output (the reconstructed / synthesized image) given \(z\) and condition \(y\).  
  \item \(D(\cdot \mid y)\) is the discriminator’s estimate (probability) that its input is “real,” given the same condition \(y\).  
  \item \(\lambda\) is a hyperparameter controlling the trade-off between adversarial realism and pixel-wise fidelity (typical values depend on the problem, e.g.\ in pix2pix, \(\lambda = 100\)). 
\end{itemize}

\subsection{Discriminator}
The Discriminator is tasked with classifying the images produced by the Generator as either real or fake. It takes a real image from the dataset (often referred to as the target image for the Generator) and provides feedback to guide the Generator toward producing more accurate images. In this work, the PatchGAN model \citep{isola2017image} is employed as the Discriminator. Unlike a traditional global classifier, PatchGAN evaluates individual patches of the image, outputting a grid of predictions rather than a single scalar value. Each element in the grid corresponds to the ``realness'' of one patch of the image under examination of the Discriminator at a time. The final loss of the Discriminator is the average over all the patch responses. Evaluating the ``realness'' of the input image in terms of its constituent patches facilitates capture of texture/ style and other high frequency components in the image. As compared to a global discriminator, it also reduces the number of parameters in the network thereby helping reduce computation cost. It also works on images with arbitrary sizes.

Prior to the down-sampling of data using PatchGAN, each input image is preprocessed with application of Zero Padding followed by batch normalization. The purpose of Zero Padding is to prevent the loss of spatial information and to facilitate the extraction of deeper features from the down-sampled output. Batch normalization is required to stabilize the learning (loss minimization) process. PatchGAN then reduces the spatial dimensions of the images to extract localized features, ensuring the model focuses on smaller regions. In this down-sampling stage, a leaky version of the Rectified Linear Unit (LeakyReLU) is applied in the convolutional layers, similar to the approach used in the Generator. The probability $D(\cdot|y)$ that the patch of the input image represented by $\cdot$ is ``real'' is evaluated through this process. The loss function $D$ of the full input image is obtained by averaging over all the patch responses. 

The Discriminator Loss function $L_D$ that is optimized during the training process is given by 
\begin{equation}
\centering
\begin{aligned}
L_{D} \; &= \; - \, \mathbb{E}_{x \sim p_{\mathrm{data}}(x \mid y)} \bigl[ \log D(x \mid y) \bigr] \\
& -\; \mathbb{E}_{z \sim p_{z}(z)} \bigl[ \log\bigl( 1 - D(G(z \mid y)\mid y)\bigr) \bigr]
\end{aligned}
\end{equation}
where the arguments of the $D$ and $G$ functions are as noted in the text following eq.(\ref{eq:total_gen_loss}) This loss is composed of two parts: one that assesses how accurately it identifies fake images (by comparing predictions to a target value of 0) and the other that measures how accurately the Discriminator identifies real images (by comparing predictions to a target value of 1).

The training procedure of these two components of the cGAN model can be outlined as follows:
\begin{itemize}
\item{The discriminator \(D\) is updated by minimizing \(L_D\), keeping \(G\) fixed (so that \(D\) becomes better at distinguishing real vs generated images).}  
\item{The generator \(G\) is updated by minimizing \(L_G\), keeping \(D\) fixed, thus pushing \(G\) to generate images that are (a) judged “real” by \(D\), and (b) close (in pixel-wise sense) to the ground-truth \(x\).} 
\end{itemize}
