\section{Network Parameters}
\begin{figure*}
	\centering
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_7.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_12.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_13.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_29.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_39.png}
\caption{Example results of image reconstruction using the cGAN model along with the II observations simulated in this work. Each row in this figure represents the results for a hypothetical fast-rotating star. \textcolor{blue}{The left panel represents the sparse II pattern obtained by the simulated observation of the star using the four IACTs illustrated in Fig.\ref{fig:teles}. This image acts as the ``condition'' part of the data input to the cGAN model. The second panel from left displays the real image, or ground truth, which the Discriminator uses to distinguish from the images generated by the Generator. The data generated for training, validation and testing of the cGAN model is a merge of this image and its II pattern presented in the left panel.} The third panel is the reconstructed image, or the predicted image, produced by the trained GAN model . \textcolor{red}{The fourth and the last panel is the difference between the ground truth and the predicted image in the $(u,v)$ plane. The uniform background color in the difference panel suggests that the network is picking up white noise from the $(u,v)$ plane.} }
	\label{fig:GAN}
\end{figure*}
\textcolor{blue}{An appropriate cGAN architecture along with a set of hyperparameters was optimized by tuning. The objective of the model, as already mentioned, was to learn to faithfully reproduce a set of sky-images of fast rotators subject to the condition that those images are consistent with their simulated II observation data. We discuss below the architecture and the hyperparameters of the cGAN model used for this task.} Given the adversarial nature of GANs, where the Generator and Discriminator engage in a minimax game, careful tuning of key parameters is critical to ensure that both networks are well-balanced for effective training.

\subsection{Data Preparation}
First, we \st{simulate} \textcolor{blue}{generate fictitious images of} fast-rotating stars, modelling them as oblate spheroids with varying radii and an oblateness ranging between 0.5 and 1. We also consider different viewing angles, assuming a linear dependence \textcolor{blue}{on declination angle on the stellar surface for generating} the effect of gravity darkening. \st{The traced ellipses result from integrating over the source's hour angle. For hyperparameter tuning and comparing different telescopes, the total observing duration is set to approximately 11.5 hours. Finally, the ellipses are plotted, converted into grayscale images, resized, and stored as raw arrays to facilitate further analysis.}

Next, Salt and Pepper noise is introduced \textcolor{blue}{into these images}; usually this is done at the rate of 0.5\% \textcolor{blue}{of the number of pixels in the image}. Then, the images are resized and their mean is subtracted. A two-dimensional Fast Fourier Transform, \st{along with a Fourier shift,} is applied, yielding a complex number for each pixel. Since II does not measure phase, the absolute value is calculated. 
%\st{(as shown in Fig.~\ref{fig:ft} on both linear and logarithmic scales for visualization)}. 

\textcolor{magenta}{\textbf{Question to discuss: Why are we calculating FFT of the images??? These images are by themselves the Fourier Transform of the sky image of the source. What do we obtain by calculating their FFT???}}


Next, \st{sparse} sampling \textcolor{blue}{of the interferometric plane} is introduced via pixel-wise multiplication of the absolute-valued Fourier-transformed image (left panel of Fig.~\ref{fig:ft}) and the \st{sparse sampling map} \textcolor{blue}{baseline tracks of} (Fig.~\ref{fig:base}) \textcolor{blue}{generated by the four IACTs due to Earth rotation during the II observation}. The result is a map in the Fourier plane featuring several ellipses, which is also referred to as the sparse sampling map (right panel of fig.~\ref{fig:ft}). This map represents the sparse sampling of the signal space 
%\st{(Fig.~\ref{fig:ft})} 
corresponding to the source (Fig.~\ref{fig:image}) observed with four telescopes (Fig.~\ref{fig:teles}). \textcolor{blue}{In essence, the right panel of fig.~\ref{fig:ft} represents the result of simulated II observation of the fictitious fast-rotator illustrated in fig.~\ref{fig:image}.}

\iffalse
\st{Finally, the pixels are normalized and converted to 8-bit integers. This image represents the sparsely sampled phaseless visibility as \st{it can be} measured with II. The image shown in right panel of fig.~\ref{fig:ft} serves as the input for the GAN, which also requires the corresponding ground truth image. Consequently, the simulated stars are resized using the same algorithm and converted to 8-bit integers to reduce bias. The GAN must have access to the ground truth corresponding to each input image; therefore, the input and ground truth images are merged side-by-side (as shown in Fig.~\ref{fig:GANinput}) and used to train the GAN. This procedure is applied to all simulated stars, with 10\% used as test data, 10\% as validation data, and the remaining 80\% as training data.}
\fi

\textcolor{blue}{Finally, we normalize the pixel values and convert them to 8-bit integers, producing an image that encodes the sparsely sampled, phase-free visibility measured by II. The image of the corresponding simulated (fictitious) star, which serves as the ``ground truth'' is processed identically to avoid any bias. These two images are merged side-by-side into a single 
image (as shown in Fig.~\ref{fig:GANinput}). The dataset thus created is split into three parts in the proportions of 80\% for training, 10\% for validation, and 10\% as a held-out test set.}



\subsection{GAN Architecture}
\textcolor{blue}{The GAN architecture used in this work is a Pix2Pix cGAN, which uses an image-to-image translation strategy with both the ground truth and the condition being images. Originally introduced by Isola et al. \cite{isola2017image}, this architecture is specifically suitable for image processing and reconstruction objectives. For instance, the TensorFlow tutorials\footnote{\url{https://www.tensorflow.org/tutorials/generative/pix2pix}} demonstrate its application to a dataset of architectural facades. This architecture has been adapted
for the phase retrieval problem at hand here \st{, some modifications are necessary}. The network is implemented using the TensorFlow library \citep{abadi2016tensorflow}, calculations are performed with scipy \citep{virtanen2020scipy}, and plots are generated with matplotlib \citep{4160265}.}

\subsection{Hyperparameter Tuning}
The cGAN model architecture used in this work employs several hyper-parameters, which are explained briefly below \citep[for a more in-depth discussion, see][]{murphy2022probabilistic}.

The learning rate of the optimizer determines how much the model updates its parameters with each iteration. A learning rate that is too small may lead to underfitting, while one that is too large can render the model unstable. Therefore, selecting an appropriate learning rate is crucial \citep{murphy2022probabilistic}. 

The kernel size refers to the dimensions of the convolutional kernel used in the network, determining how many pixels are combined to produce a new pixel. A larger kernel size can capture features spanning several pixels, but it may also incorporate unrelated features.

The amount of noise is controlled by two parameters, ``alpha" and ``beta", which indicate the percentage of pixels altered to either white or black, hence the term Salt and Pepper noise. Here, ``alpha" is applied to the real image, while ``beta" is applied to the generated image. Different ratios (``alpha/beta") can lead to varying model performance; however, our results indicate that distinct noise rates do not significantly affect the loss functions. 

The batch size defines the number of images processed simultaneously by the network. Smaller batch sizes have been observed to improve generalization \citep{prince2023understanding}. However, because a larger batch size significantly increases training time, a batch size of 1 is used.

When training GANs, one strategy to potentially boost performance is to give the Discriminator an advantage by increasing its number of training steps before returning to the Generator's training. While this can lower the Discriminator loss, as shown in Fig.~\ref{fig:Plot_discrep_loss}, it also increases training time \st{and leads to a slight rise in the Generator loss}. \textcolor{blue}{In training our model, we did not notice any significant advantage derived from this strategy. Since the generated images do not noticeably improve with additional Discriminator training, both networks are \st{typically} trained with the same number of steps.}

Finally, the degree of sparse sampling can be varied to provide the model with access to more \textcolor{blue}{active (mon-zero)} pixels. Increasing the number of telescopes results in more baselines and, consequently, more available pixels. Fig.~\ref{fig:Plot_telescopes_loss} shows the loss functions for different numbers of telescopes. There is a significant disparity in performance: \textcolor{blue}{at ``Nash point'', both the loss functions are minimized for the case of four telescopes. This is partly because the relationship between telescopes and baselines is quadratic: the coverage of the Fourier plane of Intensity Interferometry (II) scales as square of the number of telescopes employed in the II observation.} For example, the Fourier plane can be sampled along six tracks when using four telescopes, as compared to only one track if using only two telescopes. In the case of two telescopes, both the Generator and Discriminator exhibit less smooth training, as indicated by the outliers. Performance improves with three telescopes and becomes very promising with four. Overall, the degree of sparse sampling appears to have the most pronounced effect of all the hyperparameters.

All the selected hyperparameters to train the model is summarized in the given table:
\begin{table}[ht]
	\centering
	\caption{Selected hyperparameters used for training the model.}
	\label{tab:hyperparameters}
	\begin{tabular}{ll}
		\hline
		\textbf{Hyperparameter} & \textbf{Value} \\
		\hline
		Learning rate           & 2e-4 \\
		Kernel size             & 5 \\
		Alpha/Beta              & 1 \\
		Batch size              & 1 \\
		Buffer Size             & 1400 \\
		Discriminator repetition  & 1 \\
		No. of Telescope        & 4 \\
		Output Channels         & 1 \\
		Lambda                  & 100 \\
		\hline
	\end{tabular}
\end{table}

\iffalse
The process to train the model is summarized here:
\begin{figure*}[htbp]
	%\centering
	\begin{center}
		\begin{tikzpicture}[node distance=0.7cm]
			
			% Data pipeline
			\node[block, fill=blue!15] (data) {Ground Truth \\ (Create SS Images of Stellar Objects)};
			\node[block, fill=red!10, below=of data] (pre) {Simulation \\ (Intensity Interferometry (II) Data)};
			\node[block, fill=green!20, below=of pre, minimum width=5cm] (split) {Input Dataset (Merge of Ground Truth \\ and Simulation of II) Split \\ (Training, Validation, Testing)};
			
			% Training, Validation, Testing
			\node[block, fill=green!15, below left=0.6cm and 1.0cm of split] (train) {Training Set \\ (Learn Model Parameters)};
			\node[block, fill=green!10, below=of split] (val) {Validation Set \\ (Tune Hyperparameters)};
			\node[block, fill=green!05, below right=0.6cm and 1.0cm of split] (test) {Testing Set \\ (Final Evaluation)};
			
			% Model training block
			\node[block, fill=orange!20, below= 1.0cm of val, minimum width=10cm, minimum height=1.2cm] (mod) {Train the Model \\ (Generator + Discriminator)};
			
			% Generator side (left)
			\node[block, fill=yellow!15, below=2.2cm of mod, xshift=-3.5cm] (noise) {Random Noise $z$};
			\node[block, fill=blue!10, below=of noise] (gen) {Generated Image $\hat{x}$};
			\node[block, fill=purple!20, below=of gen] (gloss) {Generator Loss \\ (Update $G$)};
			
			% Discriminator side (right)
			\node[block, fill=yellow!20, below=2.2cm of mod, xshift=+3.5cm] (real) {Real Image \\ (From Training Set)};
			\node[block, fill=orange!20, below=of real] (disc) {Discriminator $D(x)$};
			\node[block, fill=red!20, below=of disc] (out) {Discriminator Output};
			\node[block, fill=purple!30, below=of out] (dloss) {Discriminator Loss \\ (Update $D$)};
			
			% Final result block
			\node[block, fill=teal!20, below= 3cm of val, minimum width=5cm] (res) {Result / Evaluation \\ (Performance Metrics: Accuracy)};
			
			% Connections
			\draw[arrow] (data) -- (pre);
			\draw[arrow] (pre) -- (split);
			%\draw[arrow] (split.south west) |- (train.north);
			\draw[arrow] (split.west) -| (train.north);
			\draw[arrow] (split.south) -- (val.north);
			\draw[arrow] (split.east) -| (test.north);
			\draw[arrow] (train.south) -- (mod.north);
			
			%Newly Introduced Connections (S Sarangi)
			\draw[arrow] (val.south) -- (mod.north);
			\draw[arrow] (mod.south) -- (res.north);
			
			% GAN arrows
			\draw[arrow] (mod.south) -- (noise.north);
			\draw[arrow] (mod.south) -- (real.north);
			\draw[arrow] (noise.south) -- (gen.north);
			\draw[arrow] (gen.south) -- (gloss.north);
			\draw[arrow] (real.south) -- (disc.north);
			\draw[arrow] (gen.east) -- ++(0.5,0) |- (disc.west);
			\draw[arrow] (disc.south) -- (out.north);
			\draw[arrow] (out.south) -- (dloss.north);
			
			% Validation & Testing to Result
			%\draw[arrow] (val.south) -- (res.north);
			\draw[arrow] (test.south) -- (res.north);
			
		\end{tikzpicture}
	\end{center}
\caption{A block diagram for the process of image reconstruction through GAN.}
\end{figure*}

\fi

\iffalse
\documentclass{article}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, positioning}

\tikzset{
  block/.style = {draw, rectangle, minimum height=1.5em, minimum width=3em, align=center},
  arrow/.style = {->, thick},
}

%\begin{document}
\fi

\iffalse
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[node distance=0.9cm]
%\begin{tikzpicure}[scale = 0.5]

% Data pipeline
\node[block, fill=blue!15] (data) {Ground Truth \\ (Create SS Images of Stellar Objects)};
\node[block, fill=red!10, below=of data] (pre) {Simulation \\ (Intensity Interferometry (II) Data)};
\node[block, fill=green!20, below=of pre, minimum width=5cm] (split) {Input Dataset (Merge of Ground Truth \\ and Simulation of II) Split \\ (Training, Validation, Testing)};

% Training, Validation, Testing
\node[block, fill=green!15, below left=0.6cm and 2.5cm of split] (train) {Training Set \\ (Learn Model Parameters)};
\node[block, fill=green!10, below=of split] (val) {Validation Set \\ (Tune Hyperparameters)};
\node[block, fill=green!05, below right=0.6cm and 2.5cm of split] (test) {Testing Set \\ (Final Evaluation)};

% Model training block
\node[block, fill=orange!20, below=3cm of val, minimum width=10cm, minimum height=1.2cm] (mod) {Train the Model \\ (Generator + Discriminator)};

% Generator side (left)
\node[block, fill=yellow!15, below=1.2cm of mod, xshift=-3.5cm] (noise) {Random Noise $z$};
\node[block, fill=blue!10, below=of noise] (gen) {Generated Image $\hat{x}$};
\node[block, fill=purple!20, below=of gen] (gloss) {Generator Loss \\ (Update $G$)};

% Discriminator side (right)
\node[block, fill=yellow!20, below=1.2cm of mod, xshift=+3.5cm] (real) {Real Image \\ (From Training Set)};
\node[block, fill=orange!20, below=of real] (disc) {Discriminator $D(x)$};
\node[block, fill=red!20, below=of disc] (out) {Discriminator Output};
\node[block, fill=purple!30, below=of out] (dloss) {Discriminator Loss \\ (Update $D$)};

% Final result block
\node[block, fill=teal!20, below=4.2cm of val, minimum width=5cm] (res) {Result / Evaluation \\ (Performance Metrics: Accuracy)};

% Connections
\draw[arrow] (data) -- (pre);
\draw[arrow] (pre) -- (split);
\draw[arrow] (split.south west) |- (train.north);
\draw[arrow] (split.south) -- (val.north);
\draw[arrow] (split.south east) |- (test.north);
\draw[arrow] (train.south) -- (mod.north);

% GAN arrows
\draw[arrow] (mod.south) -- (noise.north);
\draw[arrow] (mod.south) -- (real.north);
\draw[arrow] (noise.south) -- (gen.north);
\draw[arrow] (gen.south) -- (gloss.north);
\draw[arrow] (real.south) -- (disc.north);
\draw[arrow] (gen.east) -- ++(0.5,0) |- (disc.west);
\draw[arrow] (disc.south) -- (out.north);
\draw[arrow] (out.south) -- (dloss.north);

% Validation & Testing to Result
\draw[arrow] (val.south) -- (res.north);
\draw[arrow] (test.south) -- (res.north);

\end{tikzpicture}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{FlowDiagram_TikZ.pdf}
\caption{A block diagram for the process of image reconstruction through C-GAN.}
\label{fig:FlowDiagram}
\end{center}
\end{figure}
%\end{document}

\fi

