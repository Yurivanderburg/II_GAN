\section{Network Parameters}
\begin{figure*}
	\centering
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_7.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_12.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_13.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_29.png}
	\includegraphics[width=0.785\textwidth]{fig/testing_image/image_39.png}
\caption{Example results of image reconstruction using the cGAN model along with the II observations simulated in this work. Each row in this figure represents the results for a hypothetical fast-rotating star. The left panel represents the sparse II pattern obtained by the simulated observation of the star using the four IACTs illustrated in Fig.\ref{fig:teles}. This image acts as the ``condition'' part of the data input to the cGAN model. The second panel from left displays the real image, or ground truth, which the Discriminator uses to distinguish from the images generated by the Generator. The data generated for training, validation and testing of the cGAN model is a merge of this image and its II pattern presented in the left panel. The third panel is the reconstructed image, or the predicted image, produced by the trained GAN model . The fourth and the last panel is the difference between the ground truth and the predicted image in the $(u,v)$ plane.}
	\label{fig:GAN}
\end{figure*}
An appropriate cGAN architecture along with a set of hyperparameters was optimized by tuning. The objective of the model, as already mentioned, was to learn to faithfully reproduce a set of sky-images of fast rotators subject to the condition that those images are consistent with their simulated II observation data. We discuss below the architecture and the hyperparameters of the cGAN model used for this task. Given the adversarial nature of GANs, where the Generator and Discriminator engage in a minimax game, careful tuning of key parameters is critical to ensure that both networks are well-balanced for effective training.


\subsection{Data Preparation}\label{sec:DataPrep}
First, we generate synthetic images of rapidly rotating stars by modelling them as oblate spheroids with varying radii and oblateness parameters between 0.5 and 1. To incorporate the effect of gravity darkening, we also consider different viewing angles and assume a linear dependence on the declination angle of each point on the stellar surface. The traced ellipses result from integrating over the source's hour angle.

Next, Salt and Pepper noise is introduced into these images; usually this is done at the rate of 0.5\% of the number of pixels in the image. Then, the images are resized and their mean is subtracted. A two-dimensional Fast Fourier Transform, along with a Fourier shift, is applied, yielding a complex number for each pixel. Since II does not measure phase, the absolute value is calculated. 

Next, sampling of the interferometric plane is introduced via pixel-wise multiplication of the absolute-valued Fourier-transformed image (left panel of Fig.~\ref{fig:ft}) and the baseline tracks of (Fig.~\ref{fig:base}) generated by the four IACTs due to Earth rotation during the II observation. The result is a map in the Fourier plane featuring several ellipses, which is also referred to as the sparse sampling map (right panel of fig.~\ref{fig:ft}). This map represents the sparse sampling of the signal space 
corresponding to the source (Fig.~\ref{fig:image}) observed with four telescopes (Fig.~\ref{fig:teles}). In essence, the right panel of fig.~\ref{fig:ft} represents the result of simulated II observation of the fictitious fast-rotator illustrated in fig.~\ref{fig:image}.

Finally, we normalize the pixel values and convert them to 8-bit integers, producing an image that encodes the sparsely sampled, phase-free visibility measured by II. The image of the corresponding simulated (fictitious) star, which serves as the ``ground truth'' is processed identically to avoid any bias. These two images are merged side-by-side into a single 
image (as shown in Fig.~\ref{fig:GANinput}). The dataset thus created is split into three parts in the proportions of 80\% for training, 10\% for validation, and 10\% as the test set. This partition of the full dataset is indicated in the Fig.\ref{fig:FlowDiagram}. In the following, we refer to these three parts as the Training Set, the Validation Set and the Testing Set respectively.

\subsection{GAN Architecture}
The GAN architecture used in this work is a Pix2Pix cGAN, which uses an image-to-image translation strategy with both the ground truth and the condition being images. Originally introduced by Isola et al. \cite{isola2017image}, this architecture is specifically suitable for image processing and reconstruction objectives. For instance, the TensorFlow tutorials\footnote{\url{https://www.tensorflow.org/tutorials/generative/pix2pix}} demonstrate its application to a dataset of architectural facades. This architecture has been adapted
for the phase retrieval problem at hand here. The network is implemented using the TensorFlow library \citep{abadi2016tensorflow}, calculations are performed with scipy \citep{virtanen2020scipy}, and plots are generated with matplotlib \citep{4160265}.

\subsection{Hyperparameter Tuning}
The cGAN model architecture used in this work employs several hyper-parameters, which are explained briefly below \citep[for a more in-depth discussion, see][]{murphy2022probabilistic}.

The learning rate ($lr$) of the optimizer determines how much the model updates its parameters with each iteration. A learning rate that is too small may lead to underfitting, while one that is too large can render the model unstable. Therefore, selecting an appropriate learning rate is crucial \citep{murphy2022probabilistic}. A canonical choice in Pix2Pix and other GAN models is $lr = 2\times10^{-4}$. In our case too, we found this choice to be appropriate.

The kernel size refers to the dimensions of the convolutional kernel used in the network, determining how many pixels are combined to produce a new pixel. A larger kernel size can capture features spanning several pixels, but it may also incorporate unrelated features. Small kernel sizes are preferable in cases where target images finer details or high spatial frequency features. Since the ``ground truth'' target images in our case have longer scale gravity darkening features, we have opted for the more canonical choice of kernel size being 5.

The amount of noise is controlled by two parameters, ``alpha" and ``beta", which indicate the percentage of pixels altered to either white or black, hence the term Salt and Pepper noise. Here, ``alpha" is applied to the real image, while ``beta" is applied to the generated image. Different ratios (``alpha/beta") can lead to varying model performance; however, our results indicate that distinct noise rates do not significantly affect the loss functions. 

The batch size defines the number of images processed simultaneously by the network. Smaller batch sizes have been observed to improve generalization \citep{prince2023understanding}. However, because a larger batch size significantly increases training time, a batch size of 1 is used. Besides, in Pix2Pix cGAN implementations found in literature this choice is found to be often preferred.

Buffer size of a Pix2Pix GAN refers to a small memory of image pool of previously generated images. They are occasionally fed to the Discriminator in place of the freshly generated ones. This strategy of mixing old and new fakes  mitigates the risk of mode collapse wherein the Discriminator tends to map all or large number of generated images to only one or a few real ``ground truth'' image(s). We have chosen a fairly large buffer size (=1400) to protect the model against mode collapse.


In the training of GANs, one often-followed strategy to potentially boost performance is to give the Discriminator an advantage by increasing its number of training epochs before returning to the Generator's training. This hyperparameter is referred to as the Discriminator repetition (as seen in Table \ref{tab:hyperparameters}). While this can lower the Discriminator loss, as shown in Fig.~\ref{fig:Plot_discrep_loss}, it also increases training time. In training our model, we did not notice any significant advantage derived from this strategy. Since the generated images did not noticeably improve with additional Discriminator training, we adopted the strategy of training both the networks with equal preference (Discriminator repetition = 1).

One domain specific hyperparameter of the cGAN model presented here is the Number of Telescopes $N_T$. The degree of sparse sampling of the intensity interferometric (II) image plane can be varied to provide the model with access to more number of active (non-zero) pixels. Point to note here, is that the coverage of the Fourier interferometric plane (number of active pixels in the II image) scales with available number of baselines, and the latter scales quadratically with the number of telescopes. Fig.~\ref{fig:Plot_telescopes_loss} shows the loss functions for different numbers of telescopes. There is a significant disparity in performance of the model: at the ``Nash point'', both the loss functions are minimized for the case of four telescopes. Overall, the degree of sparse sampling appears to have the most pronounced effect of all the hyperparameters.

The hyperparameter Output Channels refers to the number of channels in the generator output (e.g., 1 for grayscale, 3 for RGB). It is worthwhile to recall that the cGAN model constructed in this work is trained on ``ground truth'' target images and the simulated II data, both in grey scales as seen in Fig.\ref{fig:GANinput}. It is natural that the output of this model will be in grey scales only. Therefore the value of this hyperparameter is set to 1. The choice of hyperparameter \(\lambda\) has been commented upon earlier.

An optimized set of hyperparameters is selected through an iterative process of training and validation. For a tentatively chosen set of the hyperparameters, the model is trained using the Training Set until the both the Discriminator and the Generator loss functions are minimized. This model, thus trained along with its model parameters, is then passed through validation using the Validation Set. This cycle of training and validation is iterated till an optimal set of hypermaraters is arrived at. During each epoch of training of the model, both the Discriminator and the Generator networks are trained for 100,000 steps. Plots of the ''Discriminator Loss'' function and the ``Generator Total Loss'' function presented in Fig.{\ref{fig:Plot_discrep_loss}} and Fig.{\ref{fig:Plot_telescopes_loss} represent the results of this training for two of the hyperparameters, namely, the Disriminator repetition (``discrep'', in short) and the Number of telescopes, respectively}. Obviously, the most compute-intensive part of this process is that of the training of the Model. The results of training and validation presented in this work were carried out on a CPU using two nodes, each with 48 threads and the entire process of training and validation required approximately 20 hours on the machine employed for this work. This iterative process of training and validation of the Model is represented schematically in the Fig.\ref{fig:FlowDiagram}. The chosen optimal set of hyperparameters is presented in the table Table-{\ref{tab:hyperparameters}.}
\begin{table}[ht]
	\centering
	\caption{Selected hyperparameters used for training the model.}
	\label{tab:hyperparameters}
	\begin{tabular}{ll}
		\hline
		\textbf{Hyperparameter} & \textbf{Value} \\
		\hline
		Learning rate           & 2e-4 \\
		Kernel size             & 5 \\
		Alpha/Beta              & 1 \\
		Batch size              & 1 \\
		Buffer Size             & 1400 \\
		Discriminator repetition  & 1 \\
		Number of Telescope        & 4 \\
		Output Channels         & 1 \\
		Lambda                  & 100 \\
		\hline
	\end{tabular}
\end{table}
This optimized and trained Model is then subjected to testing and evaluation using the Testing Set. The results of this testing and evaluation is presented in the following section.

The Pix2Pix cGAN architecture along with the choice of the values of the hyperparameters mentioned in the Table -\ref{tab:hyperparameters} and used in the implementation of this architecture constitutes the cGAN model (hereinafter referred to as ``the GAN Model'' or simply ``the Model'').